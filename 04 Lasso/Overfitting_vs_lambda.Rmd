---
title: 'Overfitting 2: varying $\lambda$ in lasso regression'
author: "Varun Boodram"
date: "December 31, 2015"
output:
  html_document:
    theme: cerulean
  pdf_document: default
---
Lasso regression jointly shrinks coefficients to avoid overfitting, and implicitly performs feature selection by setting some coefficients exactly to 0 for sufficiently large penalty strength $\lambda$ (here called "L1_penalty"). In particular, lasso takes the RSS term of standard least squares and adds a 1-norm cost of the coefficients $\Vert w \Vert$.


We want to observe the effects of varying $\lambda$. The data used in this experiment is the same sinusoid as before, $y=sin(4x)$, on which we try to fit a 16th-degree polynomial

```{r}
# create a random sample of 30 numbers in the interval [0,1)
x <- sort(runif(n = 30, min = 0, max = 1))
# evaluate y at each point x
y <- sin(4*x)
# add noise
e <- rnorm(30, 0, 1/3)
y <- y+e
# put into a data frame
data <- data.frame(x, y)
# visualise the data
plot(x = x, y = y, pch =20)
lines(x = x, y <- sin(4*x))

## poly_dataframe() accepts as input a data frame, a feature (the name of a single column in that data frame, wrapped in " "), and a degree, and returns a data frame whose consecutive columns are powers of the values of the feature, in increasing order up to the value of the entered degree
poly_dataframe <- function(dataframe, output, feature, degree){
        poly <- matrix(nrow = nrow(dataframe), ncol = degree)
        names<-vector()
        if (degree == 1){
                poly[,1] <- dataframe[[feature]]
                poly <- as.data.frame(poly)
                colnames(poly) <- "power_1"
        } else {
                columns <- vector()
                for (i in 1: degree){
                        names[i] <- paste("power_", i, sep = "")
                        poly[, i] <- dataframe[[feature]]^i
                        poly <- as.data.frame(poly)
                        colnames(poly) <- names
                        }
        }
        poly <-cbind(dataframe[[output]], poly)
        colnames(poly)[1]<-"output"
        poly
}
```


```{r}
require("glmnet")
require("pracma")
# create a function similar to ridgefit()

lassofit<-function(dataframe, output, feature, degree, l2_penalty){
  data <- poly_dataframe(dataframe = dataframe, 
                         output = output, 
                         feature = feature, 
                         degree = degree)
  x <- model.matrix(output~., -1, data = data)
  y <- data$output
  fit <- glmnet(x, y, alpha = 1, lambda = l2_penalty, )
  plot(x = data$power_1, 
       y = data$output, 
       pch=20, 
       xlab = "x", 
       ylab = "y", 
       main = paste("Degree", degree, "fit with lambda", l2_penalty, "nnz= ", nnz(as.vector(coef(fit))) ))
  points(dataframe[[feature]], 
         predict.glmnet(object = fit, newx = x), 
         type ="l", 
         col ="red", 
         lwd =3)
  print(coef(fit))
}

# try with a bunch of l2 penalties

penalties <- c(1e-25, 1e-10, 1e-6, 1e-3, 1e2)
for (i in 1: length(penalties)){
        l2_penalty <- penalties[i]
        lassofit(dataframe = data, output = "y", feature = "x", degree = 16, l2_penalty = l2_penalty)
}
```

